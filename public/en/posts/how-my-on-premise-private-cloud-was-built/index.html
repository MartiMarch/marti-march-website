<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>How my on premise private cloud was built // Martí March</title>
    <link rel="shortcut icon" href="/favicon.ico" />
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.148.2">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Marti March" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="/css/main.min.d024432de022f80a70ac780c7b5c1965b59484c9412974c923ed27741792fb9a.css" />
    

    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="How my on premise private cloud was built">
  <meta name="twitter:description" content="Preface In this post, I’m going to try to explain why and how I decided to build a private cloud on-premise using some cheap computers.
If you have in mind doing something like this, hosting your own infrastructure at home, let me confess: it means choose the hard way… Don’t say I didn’t warn you.
Chapter 1: The problem Chapter 2: The solution Energy architecture Network architecture Deploying K8S cluster Persistence of data Exposing services Chapter 3: The solution becomes the problem Chapter 4: Conclusions Chapter 1: The problem I enjoy creating new solutions and trying to make money. But I’m just one person, not a company. That means I need to optimize my spending, otherwise, I could end up homeless. Trying to build something can get very expensive, and in the IT world, it can become massively expensive. So, here’s my first problem: money (yeah, the usual one). The second problem is about intellectual property. When someone is working on a new solution, sharing it publicly without considering the legal side can be risky—especially if there’s any hope of future profit. I know, I know… I’m aware I probably won’t make a dime, who knows?">

    <meta property="og:url" content="//localhost:1313/en/posts/how-my-on-premise-private-cloud-was-built/">
  <meta property="og:site_name" content="Martí March">
  <meta property="og:title" content="How my on premise private cloud was built">
  <meta property="og:description" content="Preface In this post, I’m going to try to explain why and how I decided to build a private cloud on-premise using some cheap computers.
If you have in mind doing something like this, hosting your own infrastructure at home, let me confess: it means choose the hard way… Don’t say I didn’t warn you.
Chapter 1: The problem Chapter 2: The solution Energy architecture Network architecture Deploying K8S cluster Persistence of data Exposing services Chapter 3: The solution becomes the problem Chapter 4: Conclusions Chapter 1: The problem I enjoy creating new solutions and trying to make money. But I’m just one person, not a company. That means I need to optimize my spending, otherwise, I could end up homeless. Trying to build something can get very expensive, and in the IT world, it can become massively expensive. So, here’s my first problem: money (yeah, the usual one). The second problem is about intellectual property. When someone is working on a new solution, sharing it publicly without considering the legal side can be risky—especially if there’s any hope of future profit. I know, I know… I’m aware I probably won’t make a dime, who knows?">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="en">
    <meta property="article:published_time" content="2025-05-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-06T00:00:00+00:00">
    <meta property="article:tag" content="K8s">
    <meta property="article:tag" content="Sysadmin">
    <meta property="article:tag" content="Linux">
    <meta property="article:tag" content="Cloud">
    <meta property="article:tag" content="Devops">
    <meta property="article:tag" content="On-Premise">


  </head>
  <body>
    <header class="app-header">
      <a href="/"><img class="app-header-avatar" src="/user-photo.jpeg" alt="Marti March" /></a>
      <span class="app-header-title">Martí March</span>
      <hr class="menu-hr">
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/en/about-me">About me</a>
             - 
          
          <a class="app-header-menu-item" href="/en/posts/">Last posts</a>
             - 
          
          <a class="app-header-menu-item" href="/en/interests">Interests</a>
      </nav>
      <hr class="menu-hr">
      <p> </p>
      <div class="app-header-social">
        
          <a href="https://github.com/MartiMarch" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-github" viewBox="0 0 24 24" fill="currentColor"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
          </a>
        
          <a href="https://www.linkedin.com/in/mart%C3%AD-march" target="_blank" rel="noreferrer noopener me">
            <svg class="icon icon-brand-linkedin" viewBox="0 0 24 24" fill="currentColor"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
          </a>
        
      </div>
      <hr class="menu-hr">
      <div class="lang-switcher">
        <a href="/marti-march-website/es/articulos">ES</a>
        <span class="span-languages">|</span>
        <a href="/marti-march-website/en/posts">EN</a>
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">How my on premise private cloud was built</h1>
      <div class="post-meta">
        <div>
          <svg class="icon icon-calendar" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>
          May 6, 2025
        </div>
        <div>
          <svg class="icon icon-clock" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>clock</title><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>
          15 min read
        </div>
        <div>
          <svg class="icon icon-tag" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7.01" y2="7"></line></svg>
              <a class="tag" href="/tags/k8s/">K8s</a>
              <a class="tag" href="/tags/sysadmin/">Sysadmin</a>
              <a class="tag" href="/tags/linux/">Linux</a>
              <a class="tag" href="/tags/cloud/">Cloud</a>
              <a class="tag" href="/tags/devops/">Devops</a>
              <a class="tag" href="/tags/on-premise/">On-Premise</a>
        </div>
      </div>
    </header>
    <div class="post-content">
      <h1 id="preface">Preface</h1>
<p>In this post, I&rsquo;m going to try to explain why and how I decided to build a private cloud on-premise using some cheap computers.</p>
<p>If you have in mind doing something like this, hosting your own infrastructure at home, let me confess: it means choose the hard way&hellip; Don’t say I didn’t warn you.</p>
<ul>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#chapter-1-the-problem">Chapter 1: The problem</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#chapter-2-the-solution">Chapter 2: The solution</a>
<ul>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#energy-architecture">Energy architecture</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#network-architecture">Network architecture</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#deploying-k8s-cluster">Deploying K8S cluster</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#persistence-of-data">Persistence of data</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#exposing-services">Exposing services</a></li>
</ul>
</li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#chapter-3-the-solution-becomes-the-problem">Chapter 3: The solution becomes the problem</a></li>
<li><a href="/en/posts/how-my-on-premise-private-cloud-was-built/#chapter-4-conclusions">Chapter 4: Conclusions</a></li>
</ul>
<h1 id="chapter-1-the-problem">Chapter 1: The problem</h1>
<p>I enjoy creating new solutions and trying to make money. But I&rsquo;m just one person, not a company. That means I need to optimize my spending, otherwise, I could end up homeless. Trying to build something can get very expensive, and in
the IT world, it can become massively expensive. So, here&rsquo;s my first problem: money (yeah, the usual one). The second problem is about intellectual property. When someone is working on a new solution, sharing it publicly without considering
the legal side can be risky—especially if there’s any hope of future profit. I know, I know&hellip; I&rsquo;m aware I probably won&rsquo;t make a dime, who knows?</p>
<p>I came to the conclusion that, in order to avoid both problems, the infrastructure must be hosted on-premises. With this approach, I can skip the need of pay for an expensive cloud or any type of SaaS, keeping my projects hidden. On one hand, I don&rsquo;t need to
deal with a &ldquo;dynamic&rdquo; budget that could surprise me with nasty expenses. On the other one, I don&rsquo;t need to create a company. And that last point is key. I live in Spain, which means starting a company is expensive and involves fight with a lot of
bureaucratic headaches.</p>
<h1 id="chapter-2-the-solution">Chapter 2: The solution</h1>
<p>To achieve a functional system, a clear paradigm must be established first. What is the best option under my conditions? I&rsquo;ve decided to follow three main rules:</p>
<ul>
<li>Developed applications must be easy to port to another environment, such as a cloud provider like AWS, Azure or GCP.</li>
<li>The environment must allow me to test high availability (HA) and distributed solutions.</li>
<li>Each component that comprises the environment must be as cost-effective as possible.</li>
</ul>
<p>Technology operates with layers of abstraction, and one must decide which responsibilities are delegated to external systems and which are managed internally. That’s why I’ve chosen to work from the ground up—starting with physical machines
and building up to a Kubernetes cluster. Considering that a Kubernetes node requires at least 2 cores and 2 GB, I bought six machines. Three of them serve as control plane nodes and the other three as worker nodes. One of the benefits of using
Kubernetes is that the number and capacity of each node can vary on demand.</p>
<p>With this in mind, I chose the most suitable computer components — in this case, the cheapest ones:</p>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>CPU</th>
          <th>RAM</th>
          <th>Motherboard</th>
          <th>PSU</th>
          <th>Disk</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>master 1</td>
          <td>Ryzen 3 4100</td>
          <td>12 GB</td>
          <td>GigaByte A520M S2H</td>
          <td>550 W</td>
          <td>120GB SSD</td>
      </tr>
      <tr>
          <td>master 2</td>
          <td>Ryzen 3 4100</td>
          <td>12 GB</td>
          <td>GigaByte A520M S2H</td>
          <td>550 W</td>
          <td>120GB SSD</td>
      </tr>
      <tr>
          <td>master 3</td>
          <td>Ryzen 3 4100</td>
          <td>12 GB</td>
          <td>ASRock A320M-DVS R4.0</td>
          <td>550 W</td>
          <td>120GB SSD</td>
      </tr>
      <tr>
          <td>worker 1</td>
          <td>Ryzen 5 4500</td>
          <td>24 GB</td>
          <td>ASRock A320M-DVS R4.0</td>
          <td>550 W</td>
          <td>450GB SSD, 1TB SSD</td>
      </tr>
      <tr>
          <td>worker 2</td>
          <td>Ryzen 5 4500</td>
          <td>24 GB</td>
          <td>ASRock A320M-DVS R4.0</td>
          <td>550 W</td>
          <td>450GB SSD, 1TB SSD</td>
      </tr>
      <tr>
          <td>worker 3</td>
          <td>Ryzen 5 4500</td>
          <td>24 GB</td>
          <td>ASRock A320M-DVS R4.0</td>
          <td>550 W</td>
          <td>450GB SSD, 1TB SSD</td>
      </tr>
  </tbody>
</table>
<p>An overview of the cluster cost metrics:</p>
<h4 id="components">Components</h4>
<ul>
<li>Ryzen 3 4100: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">~51,80€</span></li>
<li>Ryzen 3 4100: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">~155€</span></li>
<li>4GB RAM: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">21€</span></li>
<li>8GB RAM: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">~18,47€</span></li>
<li>GigaByte A520M S2H: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">58,95€</span></li>
<li>ASRock A320M-DVS R4.0: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">62,51€</span></li>
<li>PSU (550W): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">~29€</span></li>
<li>120GB SSD:<style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation"> 14€</span></li>
<li>1TB SSD: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">58,99€</span></li>
</ul>
<h4 id="nodes">Nodes</h4>
<ul>
<li>Control plane cost (master*): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">193,22€</span></li>
<li>Worker node (worker*): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">374,91€</span></li>
</ul>
<h4 id="cluster-cost-sceneries">Cluster cost sceneries</h4>
<ul>
<li>Single node cluster: <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">374,91€</span></li>
<li>Minimal cluster (1 control plane and 1 worker node): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation"> 568,13€ </span></li>
<li>Worker HA cluster (1 control plane and 2 worker nodes): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation"> 943,04€ </span></li>
<li>Minimal HA cluster (2 control planes and 2 worker nodes): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation"> 1136,26€ </span></li>
<li><strong>Current HA cluster (3 control planes and 3 worker nodes): <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation"> 1704,39€ </span></strong></li>
</ul>
<p>To transform all this stuff into a real HA Kubernetes cluster, you have to manage three fundamentals: energy, data persistence and network configuration. Of course, there are many other things, like monitoring, disaster recovery, security&hellip;
However, if we take into consideration make it a &ldquo;minimal&rdquo; functional cluster, focus on these three things.</p>
<h3 id="energy-architecture">Energy architecture</h3>
<p>I reviewed the residential infrastructure dashboards to check the maximum power supply for each power line. The idea behind this is to determine how many devices can be connected to each power line without causing a power outage in the home
electrical system. To do that, I used the following formula:</p>
<style>
    .math-block {
        margin: 1em 0;
        border-left: 0.4em solid #d7ffaa;
        padding: 0.5em 0.5em;
        background-color: #272822;
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.12em;
        overflow-x: auto;
        color: #f8f8f2;
        line-height: 1.6;
    }

    .math-block .mjx-block {
        margin-bottom: 0.4em;
    }

</style>


<div class="math-block">
    
$$
amperes = \frac{watts}{volts} \rightarrow A = \frac{W}{V} \rightarrow A = \frac{W}{240V \text{(default in Spain)}}
\\
A_{\text{max psu}} = \frac{550W}{240V} = 2.29A
$$

</div>
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>A single node, performing at its maximum, gives a total of <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">2.29A</span>. But the PSU only supplies the energy that is demanded. So, to calculate the actual values, we must adjust
the formula according to the previous components at its maximum performance <em>(<a href="https://www.geeknetic.es/calculadora-fuente-alimentacion/">webpage used</a> to get component watts)</em>.</p>
<table>
  <thead>
      <tr>
          <th>Role</th>
          <th>Maximum watts</th>
          <th>Maximum amperes</th>
          <th>Power line</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>master 1</td>
          <td>154 W</td>
          <td>0.64 A</td>
          <td>Line 1</td>
      </tr>
      <tr>
          <td>master 2</td>
          <td>154 W</td>
          <td>0.64 A</td>
          <td>Line 2</td>
      </tr>
      <tr>
          <td>master 3</td>
          <td>154 W</td>
          <td>0.64 A</td>
          <td>Line 2</td>
      </tr>
      <tr>
          <td>worker 1</td>
          <td>157 W</td>
          <td>0.65 A</td>
          <td>Line 1</td>
      </tr>
      <tr>
          <td>worker 2</td>
          <td>157 W</td>
          <td>0.65 A</td>
          <td>Line 1</td>
      </tr>
      <tr>
          <td>worker 3</td>
          <td>157 W</td>
          <td>0.65 A</td>
          <td>Line 2</td>
      </tr>
      <tr>
          <td>Router</td>
          <td>~20 W</td>
          <td>0.08 A</td>
          <td>Line 1</td>
      </tr>
      <tr>
          <td>Switch</td>
          <td>~20 W</td>
          <td>0.08 A</td>
          <td>Line 1</td>
      </tr>
  </tbody>
</table>
<p>Each line supplies 16 amperes, so the final usage per line when computers are performing at <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">100%</span> would be as follows:</p>
<style>
    .math-block {
        margin: 1em 0;
        border-left: 0.4em solid #d7ffaa;
        padding: 0.5em 0.5em;
        background-color: #272822;
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.12em;
        overflow-x: auto;
        color: #f8f8f2;
        line-height: 1.6;
    }

    .math-block .mjx-block {
        margin-bottom: 0.4em;
    }

</style>


<div class="math-block">
    
$$
A_{\text{line 1}} = \frac{ W_{\text {master 1}} + W_{\text {worker1}} + W_{\text {worker 2} } + W_{\text {Router}} + W_{\text {Switch}}}{240V} \approx 2,11A
\\
\text{Line 1 usage} = \frac{ 2,11A }{ 16A } * 100 \approx 13,18\%
\\
A_{\text {line 2}} = \frac{ W_{\text {master2}} + W_{\text {master3}} + W_{\text {worker3}} }{240V} \approx 1,93A
\\
\text{Line 2 usage} = \frac{ 1,93A }{ 16A } * 100 \approx 12,06\%
$$

</div>
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>With all this information, is it possible guarantee that a blackout won&rsquo;t happen.</p>
<h3 id="network-architecture">Network architecture</h3>
<p>There are many ways to design network topologies. In this case, I decided to create a new isolated physical subnet to avoid interference between the cluster and domestic networks. By using an intermediate router, I created a WAN-to-LAN setup:
subnet 192.168.1.0/24 remains as the domestic network, and 192.168.2.0/24 as cluster subnet. To maintain a global IP coherency, nodes are configured according to their role:</p>
<ul>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.0</span>: Unassigned by now (<em>it would be another router to enable HA</em>)</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.1</span>: IP router</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.2 - 192.168.2.49</span>: DHCP range</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.50 - 192.168.2.59</span>: Kubernetes control plane nodes</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.60</span>: Virtual IP assigned to control plane HA</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.61 - 192.168.2.89</span>: Kubernetes worker nodes</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.90 - 192.168.2.99</span>: Reserved for persistence storage servers (<em>e.g. NAS</em>)</li>
<li><style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.100 - 192.168.2.255</span>: Unassigned</li>
</ul>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/network-topology-1.png">
</div>
<p>To build a HA kubernetes cluster, workers must communicate with a control plane, but, if it goes down, another must take over as the leader (<em>RAFT algorithm</em>).
To enable that, a virtual IP address is needed. It is commonly implemented combining HaProxy with KeepAlived.</p>
<p>It is what happens when a control node goes down:</p>
<ol>
<li>KeepAlived assigns a leader role among the masters in the available pool</li>
</ol>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/keepalive-haproxy_1.png">
</div>
<ol start="2">
<li>The virtual IP is associated with the master node in question and the worker nodes attack that control plane</li>
</ol>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/keepalive-haproxy_2.png">
</div>
<ol start="3">
<li>A control plane goes down</li>
</ol>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/keepalive-haproxy_3.png">
</div>
<ol start="4">
<li>KeepAlived negotiates the leader role again with the remaining master nodes and associates the virtual IP with it, the worker nodes attack that control plane</li>
</ol>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/keepalive-haproxy_4.png">
</div>
<p>HaProxy and KeepAlived must be installed on each control plane. Installation steps are the same, the only difference is the configuration files of haproxy and keepalived.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>apt -y update
</span></span><span style="display:flex;"><span>apt -y install vim
</span></span><span style="display:flex;"><span>apt -y install keepalived
</span></span><span style="display:flex;"><span>apt -y install heproxy
</span></span></code></pre></div><p>Content of /etc/haproxy/haproxy.cfg file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>global
</span></span><span style="display:flex;"><span>log /dev/log  local0 warning
</span></span><span style="display:flex;"><span>chroot      /var/lib/haproxy
</span></span><span style="display:flex;"><span>pidfile     /var/run/haproxy.pid
</span></span><span style="display:flex;"><span>maxconn     4000
</span></span><span style="display:flex;"><span>user        haproxy
</span></span><span style="display:flex;"><span>group       haproxy
</span></span><span style="display:flex;"><span>daemon
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stats socket /var/lib/haproxy/stats
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>defaults
</span></span><span style="display:flex;"><span>  log global
</span></span><span style="display:flex;"><span>  option  httplog
</span></span><span style="display:flex;"><span>  option  dontlognull
</span></span><span style="display:flex;"><span>  timeout connect 5000
</span></span><span style="display:flex;"><span>  timeout client 50000
</span></span><span style="display:flex;"><span>  timeout server 50000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>frontend kube-apiserver
</span></span><span style="display:flex;"><span>  # This port will be used by worker nodes to communicate 
</span></span><span style="display:flex;"><span>  # Don&#39;t set 6443, as it would conflit with the control plane&#39;s kubeadm service
</span></span><span style="display:flex;"><span>  bind *:7443
</span></span><span style="display:flex;"><span>  mode tcp
</span></span><span style="display:flex;"><span>  option tcplog
</span></span><span style="display:flex;"><span>  default_backend kube-apiserver
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>backend kube-apiserver
</span></span><span style="display:flex;"><span>  mode tcp
</span></span><span style="display:flex;"><span>  option tcplog
</span></span><span style="display:flex;"><span>  option tcp-check
</span></span><span style="display:flex;"><span>  balance roundrobin
</span></span><span style="display:flex;"><span>  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
</span></span><span style="display:flex;"><span>  # Poll of control planes that form the pool
</span></span><span style="display:flex;"><span>  server kube-apiserver-1 192.168.2.51:6443 check
</span></span><span style="display:flex;"><span>  server kube-apiserver-2 192.168.2.52:6443 check
</span></span><span style="display:flex;"><span>  server kube-apiserver-3 192.168.2.53:6443 check
</span></span></code></pre></div><p>Content of /etc/keepalived/keepalived.conf</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>global_defs {
</span></span><span style="display:flex;"><span>  notification_email {
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  router_id LVS_DEVEL
</span></span><span style="display:flex;"><span>  vrrp_skip_check_adv_addr
</span></span><span style="display:flex;"><span>  vrrp_garp_interval 0
</span></span><span style="display:flex;"><span>  vrrp_gna_interval 0
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vrrp_script chk_haproxy {
</span></span><span style="display:flex;"><span>  script &#34;killall -0 haproxy&#34;
</span></span><span style="display:flex;"><span>  interval 2
</span></span><span style="display:flex;"><span>  weight 2
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vrrp_instance haproxy-vip {
</span></span><span style="display:flex;"><span>  state BACKUP
</span></span><span style="display:flex;"><span>  priority 100
</span></span><span style="display:flex;"><span>  interface enp4s0
</span></span><span style="display:flex;"><span>  virtual_router_id 60
</span></span><span style="display:flex;"><span>  advert_int 1
</span></span><span style="display:flex;"><span>  authentication {
</span></span><span style="display:flex;"><span>  auth_type PASS
</span></span><span style="display:flex;"><span>    auth_pass 1111
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  # Control plane IP
</span></span><span style="display:flex;"><span>  unicast_src_ip 192.168.2.51
</span></span><span style="display:flex;"><span>  unicast_peer {
</span></span><span style="display:flex;"><span>    # IP&#39;s of other control plane
</span></span><span style="display:flex;"><span>    192.168.2.52
</span></span><span style="display:flex;"><span>    192.168.2.53
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  virtual_ipaddress {
</span></span><span style="display:flex;"><span>    # IP addres of virtual control plane
</span></span><span style="display:flex;"><span>    192.168.2.60/24
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  track_script {
</span></span><span style="display:flex;"><span>    chk_haproxy
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>As last step, enable both services:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl enable haproxy
</span></span><span style="display:flex;"><span>systemctl enable keepalived
</span></span><span style="display:flex;"><span>systemctl start haproxy
</span></span><span style="display:flex;"><span>systemctl start keepalived
</span></span></code></pre></div><h3 id="deploying-k8s-cluster">Deploying K8S cluster</h3>
<p>To deploy the Kubernetes cluster, I used <a href="https://www.kubesphere.io/docs/v3.3/installing-on-linux/high-availability-configurations/set-up-ha-cluster-using-keepalived-haproxy/">KubeKey</a>, a tool developed by the KubeSphere community that simplifies the installation and management of Kubernetes clusters, including high availability setups.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -sfL https://get-kk.kubesphere.io | VERSION<span style="color:#f92672">={</span>versión<span style="color:#f92672">}</span> sh -
</span></span></code></pre></div><p>To configure the cluster, a YAML definition is required, including all node IPs and their respective roles.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kubekey.kubesphere.io/v1alpha2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">sample</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Nodes that will make up the cluster, it is necessary to connect the script via SSH</span>
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: master1, address: 192.168.2.51, internalAddress: 192.168.2.51, user: master1, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: master2, address: 192.168.2.52, internalAddress: 192.168.2.52, user: master2, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: master3, address: 192.168.2.53, internalAddress: 192.168.2.53, user: master3, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: worker1, address: 192.168.2.61, internalAddress: 192.168.2.61, user: worker1, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: worker2, address: 192.168.2.62, internalAddress: 192.168.2.62, user: worker2, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  - {<span style="color:#f92672">name: worker3, address: 192.168.2.63, internalAddress: 192.168.2.63, user: worker3, password</span>: <span style="color:#e6db74">&#34;&#34;</span>}
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">roleGroups</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">etcd</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Control plane nodes alias</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master1</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master2</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">control-plane</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master1</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master2</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">master3</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">worker</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Worker nodes alias</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">worker1</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">worker2</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">worker3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">controlPlaneEndpoint</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">domain</span>: <span style="color:#ae81ff">cluster.noprod.local</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Virtual IP address created previously</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">address</span>: <span style="color:#e6db74">&#34;192.168.2.60&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Port defined within HaProxy configuration</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">7443</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kubernetes</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">version</span>: <span style="color:#ae81ff">v1.23.10</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">clusterName</span>: <span style="color:#ae81ff">cluster.noprod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">autoRenewCerts</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Container orchestrator</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">containerManager</span>: <span style="color:#ae81ff">docker</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">etcd</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">kubekey</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">network</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Kubernetes network plugin </span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">plugin</span>: <span style="color:#ae81ff">flannel</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Virtual subnet that will use the cluster. It must not conflict with other subnets (in my case, 192.168.2.0/24).</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubePodsCIDR</span>: <span style="color:#ae81ff">10.233.64.0</span><span style="color:#ae81ff">/18</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubeServiceCIDR</span>: <span style="color:#ae81ff">10.233.0.0</span><span style="color:#ae81ff">/18</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">multusCNI</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">registry</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">privateRegistry</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespaceOverride</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">registryMirrors</span>: []
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">insecureRegistries</span>: []
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">addons</span>: []
</span></span></code></pre></div><p>To deploy the cluster use the Kube Key CLI, it will use ssh to configure all the nodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>chmod +x kk
</span></span><span style="display:flex;"><span>./kk create cluster -f <span style="color:#f92672">{</span>kube key cluster configuration<span style="color:#f92672">}</span>.yaml
</span></span></code></pre></div><h3 id="persistence-of-data">Persistence of data</h3>
<p>Persistence of data is a challenging aspect in distributed systems. I have tested many solutions, and the one I chose is to keep the data replicated. In this approach, data is treated like a pod: you configure a desired number of replicas, and the
system ensures it remains replicated across the disks of worker nodes. If one node goes down, a persistent volume will reattach the data from another available node while the healthy data is cloned. That is why an extra 1TB SSD has been added to each
worker node. All of this is achieved using Longhorn, steps to install it:</p>
<ol>
<li>Disable multipath on each worker node</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>vi /etc/multipath.conf
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>blacklist {
</span></span><span style="display:flex;"><span>    devnode &#34;^sd[a-z0-9]+&#34;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl enable multipathd
</span></span><span style="display:flex;"><span>systemctl start multipathd
</span></span><span style="display:flex;"><span>multipath -ll <span style="color:#75715e"># To check if mounts are working with multipath</span>
</span></span></code></pre></div><ol start="2">
<li>Longhorn installation:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm repo add longhorn https://charts.longhorn.io
</span></span><span style="display:flex;"><span>helm repo update
</span></span><span style="display:flex;"><span>kubectl create namespace longhorn-system
</span></span><span style="display:flex;"><span>helm install longhorn longhorn/longhorn --namespace longhorn-system
</span></span></code></pre></div><ol start="3">
<li>Worker node disks configuration</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>lsblk
</span></span><span style="display:flex;"><span>fdisk /dev/vdb <span style="color:#75715e"># n, p, 1, w</span>
</span></span><span style="display:flex;"><span>mkfs.ext4 /dev/&lt;SSD disk name&gt;
</span></span><span style="display:flex;"><span>mkdir /mnt/ssd-1 <span style="color:#75715e"># Can be set orher name</span>
</span></span><span style="display:flex;"><span>mount /dev/vdb1 /mnt/ssd-1
</span></span><span style="display:flex;"><span>vi /etc/fstab <span style="color:#75715e"># Append the next line to /etc/fstab -&gt; &#34;/dev/vdb1 /mnt/ssd-1 ext4 defaults 0 0&#34;</span>
</span></span><span style="display:flex;"><span>df -h
</span></span></code></pre></div><style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/longhorn_1.png">
</div>
<ol start="4">
<li>Attach Longhorn storage class to PVC</li>
</ol>
<p>To use longhorn storage class add the next definition into the PVC and PV:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># PV manifest example</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolume</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ha-pv-data</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">capacity</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">20Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">claimRef</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ha-pvc-data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">infra</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">driver</span>: <span style="color:#ae81ff">driver.longhorn.io</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">fsType</span>: <span style="color:#ae81ff">ext4</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumeHandle</span>: <span style="color:#ae81ff">gitea-ha</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">longhorn</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumeMode</span>: <span style="color:#ae81ff">Filesystem</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># PVC manifest example</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ha-pvc-data</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">infra</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">20Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">longhorn</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumeMode</span>: <span style="color:#ae81ff">Filesystem</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumeName</span>: <span style="color:#ae81ff">gitea-ha-pv-data</span>
</span></span></code></pre></div><p>Volumes can be created by demand or manually from longhorn UI.</p>
<h3 id="exposing-services">Exposing services</h3>
<p>In the previous guide, we created a cool HA Kubernetes cluster. Nevertheless, without exposing services, it&rsquo;s not very useful.</p>
<p>There are two main options to expose services outside the cluster: via NodePort and LoadBalancer. Using a NodePort doesn&rsquo;t make much sense in an HA setup because the service is tied to a single node, which can be a single point of failure. One
approach could be to set up a custom load balancer with HAProxy and KeepAlived using a dedicated pool of machines. While that could work, it also means higher costs due to the need for at least two additional servers. Instead, a better option
might be to use the worker nodes themselves as load balancers, leveraging OpenELB.</p>
<p>I chose <a href="https://openelb.io/docs/concepts/layer-2-mode/">OpenELB Layer 2</a> mode to assign a virtual IP to an Ingress. Here&rsquo;s how it works:</p>
<ul>
<li>OpenELB assigns a virtual IP from the real subnet (not from the Kubernetes overlay network) to the service</li>
<li>The router adds the IP to its ARP table</li>
<li>The virtual IP is bound to a worker node. If that node goes down, OpenELB reassigns the IP to another healthy node</li>
</ul>
<p>OpenELB can be installed easily using Kubernetes manifests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.0/deploy/static/provider/cloud/deploy.yaml
</span></span><span style="display:flex;"><span>kubectl apply -f https://raw.githubusercontent.com/openelb/openelb/master/deploy/openelb.yaml
</span></span><span style="display:flex;"><span>kubectl -n openelb-system get pod
</span></span></code></pre></div><p>The subnet used by OpenELB to assign virtual IPs must be defined using Eip CRD:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">network.kubesphere.io/v1alpha2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Eip</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">layer2-eip</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Real subnet</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">address</span>: <span style="color:#ae81ff">192.168.2.80-192.168.2.200</span> 
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Physical interface of worker nodes</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">interface</span>: <span style="color:#ae81ff">enp6s0</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">layer2</span>
</span></span></code></pre></div><p>Now the LoadBalancer service can be created. Once it is deployed, OpenELB will assign an IP from the previously defined subnet range.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">layer2-svc</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">lb.kubesphere.io/v1alpha1</span>: <span style="color:#ae81ff">openelb</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol.openelb.kubesphere.io/v1alpha1</span>: <span style="color:#ae81ff">layer2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">eip.openelb.kubesphere.io/v1alpha2</span>: <span style="color:#ae81ff">layer2-eip</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">layer2-openelb</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">LoadBalancer</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8080</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">externalTrafficPolicy</span>: <span style="color:#ae81ff">Cluster</span>
</span></span></code></pre></div><p>In this case, it has set <style>
    .math-operation {
        font-family: 'Latin Modern Roman', serif;
        font-size: 1.1em;
        padding: 2px 6px;
        border-radius: 4px;
        color: white;
    }
</style>

<span class="math-operation">192.168.2.80</span> as the external IP because it&rsquo;s the first address in the EIP address range.</p>
<style>
    .img-center {
        text-align: center;
        margin: 1em 0;
    }
    .img-center img {
        display: inline-block;
    }
</style>

<div class="img-center">
    
    
    <img src="/posts/how-my-on-premise-private-cloud-was-built/openelb_1.png">
</div>
<p>The last step to achieve a solid solution is to secure access to the Ingress using a custom CA. It can be done with cert-manager, a built-in Kubernetes service that orchestrates certificates.</p>
<p>It requires creating a key to sing certificates with the CA.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># Creates the private key</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">openssl genrsa -out ca.key 4096</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Signs the certificate</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">openssl req -new -x509 -sha256 -days 365 -key ca.key -out ca.crt</span>
</span></span></code></pre></div><p>cert-manager store CA certificate through a ClusterIssuer CRD.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">cert-manager.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterIssuer</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">selfsigned</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ca</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">ca-cert-manager</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ca-cert-manager</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">cert-manager</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls.crt</span>: <span style="color:#75715e"># Base64-encoded content of ca.crt (e.g., cat ca.crt | base64 -w 0)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls.key</span>: <span style="color:#75715e"># Base64-encoded content of ca.key (e.g., cat ca.key | base64 -w 0)</span>
</span></span></code></pre></div><p>To expose the service outside the cluster, we have to combine Nginx and cert-manager within a kubernetes Ingress manifest.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># This annotation tells cert-manager to use the specified ClusterIssuer (in this case, a CA-based issuer)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># to automatically issue a TLS certificate for this Ingress.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-cluster-issuer</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ha-gitea-ha</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">infra</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Specifies that this Ingress should be handled by the NGINX Ingress Controller</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingressClassName</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">pre.gitea.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea-ha-gitea-ha</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">number</span>: <span style="color:#ae81ff">3000</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># cert-manager will store the issued TLS certificate in this secret.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># If it doesn&#39;t exist, cert-manager will create it and renew it automatically.</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">pre.gitea.com</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">gitea-ha-gitea-ha-tls</span>
</span></span></code></pre></div><h1 id="chapter-3-the-solution-becomes-the-problem">Chapter 3: The solution becomes the problem</h1>
<p>Following this guideline, it almost feels like anyone could build a basic HA Kubernetes cluster with a pool of toasters. Looks easy, right? But I made it clear at the beginning of this post: creating and maintaining a Kubernetes cluster is hard—especially
when it&rsquo;s hosted on-premise with as few resources as possible. I’ve had to deal with a lot of problems just to keep the environment running—most of them the result of my own bad decisions.</p>
<p>At one point, the worker nodes started rebooting randomly with a kernel memory segmentation exception. I spent a lot of time trying to figure out the cause of the error. After a few weeks without finding the origin of the issue, I threw in the towel. I
decided to copy the data from the persistent volumes to my personal computer and reinstall the entire cluster. But&hellip; it showed the same error. I started thinking about all the changes I had made recently, and then I realized the real issue: a RAM slot
I had added not long ago. It was running at a different frequency than the others. Yes, I spent a lot of time&hellip; and the fix was embarrassingly simple.</p>
<p>Another issue was using a Molex-to-SATA connection. In the beginning, before installing Longhorn, all the data was stored on a separate host. It acted as the Kubernetes storage controller using NFS. The motherboard on that host only had six SATA slots,
so I used a Molex-to-SATA adapter to increase the number of disks in the RAID array. Eventually, one of the drives in the array failed, and I’m pretty sure the cause was the unstable power provided by that Molex connection. It seemed like a practical
workaround at the time, but it ended up compromising the reliability of the whole setup.</p>
<h1 id="chapter-4-conclusions">Chapter 4: Conclusions</h1>
<p>As an engineer, you must carefully evaluate whether an on-premise Kubernetes cluster is the best approach to achieve your goals. The list of problems is endless, and dealing with them drains a lot of time and energy. Nevertheless, if you decide to
build one, the knowledge you gain will be incredibly valuable. The lessons learned from working through the challenges of managing your own cluster are indispensable and will help shape your decision-making in future projects.</p>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
